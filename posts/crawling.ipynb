{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1e6224c6-9a64-48a7-a53c-c72615abe349",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Static and Dynamic Web Crawling with R & Python\"\n",
    "author: \"sw1kwon\"\n",
    "date: \"04/05/2025\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66a417-869a-4966-bdad-5214e9e713d6",
   "metadata": {},
   "source": [
    "# Introduction to Web Crawling\n",
    "\n",
    "- **Web crawling**: Navigating and collecting data from across multiple web pages  \n",
    "- **Web scraping**: Extracting specific information from a single page  \n",
    "- Depending on the target, crawling is categorized as **static (HTML-based)** or **dynamic (JavaScript-based)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403f9c9-cb18-4932-ba69-2d76d9209079",
   "metadata": {},
   "source": [
    "# Web Crawling with R – Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96ddf3-a9f7-4180-9bde-35f03578e6d6",
   "metadata": {},
   "source": [
    "## Static Web Crawling (`rvest`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9a8af-f73d-4ba8-9cf1-463c7ae645ed",
   "metadata": {},
   "source": [
    "- Use the `rvest` package to extract content directly from HTML pages  \n",
    "- Functions like `read_html()`, `html_nodes()`, and `html_text()` help collect text, links, etc.  \n",
    "- Tables built with `<table>` tags can be converted into data frames using `html_table()`  \n",
    "- Use `dplyr` for cleaning the data and `write.csv()` to save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eea80-3e8a-4beb-a656-59cef9eeb053",
   "metadata": {},
   "source": [
    "## API-Based Data Collection (`httr`, `jsonlite`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ee868-497f-4d7b-956b-bf2ada4d03a4",
   "metadata": {},
   "source": [
    "- APIs provide structured data (JSON/XML) directly from the server  \n",
    "- Use `httr::GET()` to request data and `jsonlite::fromJSON()` to parse the JSON response  \n",
    "- API-based collection is faster and more stable than HTML scraping  \n",
    "- You can extract fields like article titles, links, and dates, and save them as CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336ce18-bf99-497c-ac71-406446658c5b",
   "metadata": {},
   "source": [
    "## Dynamic Crawling with JavaScript (`RSelenium`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018c243-baca-4e20-828e-753cd7183ff5",
   "metadata": {},
   "source": [
    "- Pages rendered by JavaScript cannot be scraped using `rvest`  \n",
    "- Use `RSelenium` to automate a browser and interact with JavaScript-based pages  \n",
    "- Simulate user actions like clicking buttons, scrolling down, or handling infinite scroll  \n",
    "- Requires setting up a web driver, and may depend on the system configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653a0ea-4275-43fe-93ea-3b900b636ad9",
   "metadata": {},
   "source": [
    "## Handling Login and Sessions (`httr::POST()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05ca89-791c-411b-8034-d5f850434dc3",
   "metadata": {},
   "source": [
    "- Some websites require login before data can be accessed  \n",
    "- Simulate login with `httr::POST()` and maintain session cookies  \n",
    "- Note: Many modern websites use CAPTCHA or 2FA, so automatic login may be blocked  \n",
    "- When possible, prefer official APIs with proper authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cbc1f-b4d1-48c5-b6cd-fa3a2a20e7e8",
   "metadata": {},
   "source": [
    "## Automation and Real-World Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2b89f-a5c1-43d4-a8c9-30e6b9a0389b",
   "metadata": {},
   "source": [
    "- To run crawlers regularly, you need automation tools  \n",
    "- **Linux**: Use `cronR`; **Windows**: Use `taskscheduleR`  \n",
    "- Example projects:\n",
    "  - Daily crawling of trending news, saved to a cumulative CSV\n",
    "  - Monitoring product price changes or trending keywords  \n",
    "- When automating, also consider logging, date-tracking, and duplicate removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205c39a-65db-44d2-9104-291e67247c03",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57e2ec-55ff-47e4-aca5-73e3b97eeada",
   "metadata": {},
   "source": [
    "- **Static scraping**: `rvest`, `html_table`, `dplyr`  \n",
    "- **API-based**: `httr::GET()`, `jsonlite::fromJSON()`  \n",
    "- **Dynamic pages**: `RSelenium` for JavaScript interactions  \n",
    "- **Login/session handling**: `httr::POST()`  \n",
    "- **Automation**: `cronR` (Linux), `taskscheduleR` (Windows)  \n",
    "- All techniques come together in real-world data collection projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc75036-19f8-452a-9a50-18f319f7a610",
   "metadata": {},
   "source": [
    "# Web Crawling with Python – Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a8d70-792c-4533-88ca-8b06796a7847",
   "metadata": {},
   "source": [
    "## Static Web Crawling (`requests`, `BeautifulSoup`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0749810-4ca2-4e8e-ab7a-4bb7ced5d5ba",
   "metadata": {},
   "source": [
    "- Use `requests` to fetch HTML pages and `BeautifulSoup` to parse them  \n",
    "- Extract text, links, or tables based on HTML tags and class structures  \n",
    "- Use `pandas.read_html()` for tables, or manually parse with `BeautifulSoup`  \n",
    "- Clean data using `pandas` and save with `.to_csv()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc023b14-a0f1-4050-a58c-b4f004b63f0e",
   "metadata": {},
   "source": [
    "## API-Based Data Collection (`requests`, `json`, `pandas`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1d26e-6d64-4598-a9c7-2f49f2698fbb",
   "metadata": {},
   "source": [
    "- REST APIs provide structured data (JSON/XML) directly  \n",
    "- Use `requests.get()` and `response.json()` or `json.loads()` to parse responses  \n",
    "- Convert JSON to `pandas.DataFrame` for analysis and export  \n",
    "- More stable and faster than HTML scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0205203-16ad-4b60-b995-8c37eedfb6af",
   "metadata": {},
   "source": [
    "## JavaScript-Based Dynamic Crawling (`Selenium`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd12edd-d7b5-4810-9196-a9031aabeff0",
   "metadata": {},
   "source": [
    "- JavaScript-rendered pages can’t be scraped with `requests` or `BeautifulSoup`  \n",
    "- Use `Selenium` to simulate browser actions (clicks, scrolling, etc.)  \n",
    "- Extract final content using `driver.page_source` after the page loads  \n",
    "- Ideal for infinite scrolls, pop-ups, and dynamic interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0569435-77b3-405e-80b3-0e93713e7c61",
   "metadata": {},
   "source": [
    "## Login and Session Management (`requests.Session()` or `Selenium`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07946d3c-232a-4614-838e-80ac6bdc1396",
   "metadata": {},
   "source": [
    "- Use `requests.Session()` to maintain cookies and login credentials  \n",
    "- For complex interactions, automate login via `Selenium`  \n",
    "- Beware of CAPTCHA or multi-factor authentication – prefer APIs if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74019f50-46ab-4dc5-980c-a1e1d8e3b53f",
   "metadata": {},
   "source": [
    "## Automation and Real-World Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f7c5a-112c-4471-9351-c490a530184d",
   "metadata": {},
   "source": [
    "- Use schedulers or job runners to automate crawling  \n",
    "- Tools: `schedule`, `APScheduler`, `cron` (Linux), Task Scheduler (Windows)  \n",
    "- Example use cases:\n",
    "  - Daily news scraping into CSV  \n",
    "  - Monitoring product price trends or real-time keywords  \n",
    "- Include logging, duplicate checks, and date-stamping for reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5fd21-9db9-4b90-b6d1-3954f71d8edc",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03069b1-68de-42dd-9ea1-a4bd42926fc2",
   "metadata": {},
   "source": [
    "- **Static scraping**: `requests`, `BeautifulSoup`, `pandas`  \n",
    "- **API collection**: `requests`, `json`, `pandas`  \n",
    "- **Dynamic scraping**: `Selenium` for full browser interaction  \n",
    "- **Login/session**: `requests.Session()` or automated login via `Selenium`  \n",
    "- **Automation**: `schedule`, `APScheduler`, `cron`, Task Scheduler  \n",
    "- Together, these tools can power scalable, real-world web crawling workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d7002-9f08-4ea8-bff2-fa05612358bf",
   "metadata": {},
   "source": [
    "# Web Crawling: R vs Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452eb9c9-ee7c-4e1f-b420-cb62eeadcb64",
   "metadata": {},
   "source": [
    "| Feature              | R                                                | Python                                                 |\n",
    "|----------------------|--------------------------------------------------|---------------------------------------------------------|\n",
    "| **Static Crawling**  | `rvest`, `html_table`, `dplyr`                   | `requests`, `BeautifulSoup`, `pandas`                  |\n",
    "| **Table Extraction** | `html_table()` + `dplyr` for cleaning            | `pandas.read_html()` or manual parsing with BeautifulSoup |\n",
    "| **API Requests**     | `httr::GET()` + `jsonlite::fromJSON()`          | `requests.get()` + `response.json()` or `json.loads()` |\n",
    "| **Dynamic Crawling** | `RSelenium`                                      | `Selenium`                                              |\n",
    "| **Login Handling**   | `httr::POST()` + session cookies                 | `requests.Session()` or login via `Selenium`           |\n",
    "| **Automation Tools** | `cronR` (Linux), `taskscheduleR` (Windows)       | `cron`, `schedule`, `APScheduler`, Windows Task Scheduler |\n",
    "| **Strengths**        | Strong for statistics and quick data cleaning    | Broad ecosystem, powerful libraries, large community   |\n",
    "| **Weaknesses**       | Setup can be tricky; less real-time flexibility  | Steeper learning curve when combining with web tech    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35752ec-c4fe-461e-8e94-79f9bed94122",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e4ea2-1dc5-4c88-b2f7-1ffeab99ec93",
   "metadata": {},
   "source": [
    "- **R** is great for statistical workflows and lightweight crawling combined with data wrangling.\n",
    "- **Python** excels in scalable automation, complex website handling, and API-based data pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
