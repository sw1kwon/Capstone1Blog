{
 "cells": [
  {
   "cell_type": "raw",
   "id": "69e4932c-57de-4a56-a2a7-73e92803bee6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Static and Dynamic Web Crawling with R & Python\"\n",
    "author: \"sw1kwon\"\n",
    "date: \"04/05/2025\"\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3f39f-d28b-4788-a800-c88c59810020",
   "metadata": {},
   "source": [
    "# 웹 크롤링 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41495d-f6a4-44db-8738-7f6d23c1851e",
   "metadata": {},
   "source": [
    "- **웹 크롤링**: 여러 웹페이지를 탐색하며 데이터를 수집하는 작업  \n",
    "- **웹 스크래핑**: 한 웹페이지에서 필요한 정보를 추출하는 작업  \n",
    "- 대상에 따라 **정적 크롤링**(HTML 기반)과 **동적 크롤링(JavaScript 기반)**으로 구분됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e359a3a3-8506-4bf4-aa66-c114736facf2",
   "metadata": {},
   "source": [
    "# R을 활용한 웹 크롤링 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778bf0e-ad7a-4c98-9a15-286f8d6abf42",
   "metadata": {},
   "source": [
    "## 정적 웹 크롤링 (`rvest`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cdc912-8fa1-4478-b38f-c1247217fa07",
   "metadata": {},
   "source": [
    "- `rvest` 패키지를 활용하여 HTML 페이지에서 직접 콘텐츠를 추출  \n",
    "- `read_html()`, `html_nodes()`, `html_text()` 등의 함수로 텍스트, 링크 등을 수집  \n",
    "- `<table>` 태그로 구성된 표는 `html_table()`을 사용해 데이터프레임으로 변환 가능  \n",
    "- 수집한 데이터는 `dplyr`로 정제하고, `write.csv()`로 저장할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523a240-7f4a-4827-9873-31afa86c0a15",
   "metadata": {},
   "source": [
    "## API 기반 데이터 수집 (`httr`, `jsonlite`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d694803-6151-43b3-b362-1313cd2b7b1e",
   "metadata": {},
   "source": [
    "- API는 서버로부터 구조화된 데이터(JSON/XML)를 직접 제공  \n",
    "- `httr::GET()`으로 요청하고, `jsonlite::fromJSON()`으로 JSON 응답을 파싱  \n",
    "- HTML 스크래핑보다 더 빠르고 안정적임  \n",
    "- 기사 제목, 링크, 날짜 등의 필드를 추출하여 CSV로 저장 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e810bf1-5916-4138-b26a-d7ca8d5147dd",
   "metadata": {},
   "source": [
    "## JavaScript 기반 동적 크롤링 (`RSelenium`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40361f3-2577-44ee-bbc0-3097e5bd2306",
   "metadata": {},
   "source": [
    "- JavaScript로 렌더링되는 페이지는 `rvest`로는 수집할 수 없음  \n",
    "- `RSelenium`을 활용하여 브라우저를 자동화하고 동적 요소에 접근 가능  \n",
    "- 버튼 클릭, 스크롤 다운, 무한 스크롤 등의 사용자 동작을 시뮬레이션 가능  \n",
    "- 웹 드라이버 설정이 필요하며 시스템 환경에 따라 다를 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0a990-1e3f-49c3-9d26-0d214f2d583e",
   "metadata": {},
   "source": [
    "## 로그인 및 세션 유지 처리 (`httr::POST()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66be4dc-30a9-41c5-bbbf-215e6eded2a0",
   "metadata": {},
   "source": [
    "- 일부 웹사이트는 로그인 후에만 데이터 접근이 가능함  \n",
    "- `httr::POST()`를 통해 로그인 시도 후 세션 쿠키를 유지하며 접근 가능  \n",
    "- 단, 많은 웹사이트가 CAPTCHA나 2단계 인증을 사용하여 자동 로그인을 차단함  \n",
    "- 가능하다면 공식 API를 통한 인증 방식 사용을 권장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7b7ac-5376-42da-afc5-fc832bd6e9ee",
   "metadata": {},
   "source": [
    "## 자동화 및 실전 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a7773-ae40-416b-a88a-8850d42d92cc",
   "metadata": {},
   "source": [
    "- 크롤링을 정기적으로 실행하려면 자동화 도구가 필요함  \n",
    "- **Linux**: `cronR` 사용, **Windows**: `taskscheduleR` 사용  \n",
    "- 예시 프로젝트:\n",
    "  - 매일 인기 뉴스 수집 및 CSV 누적 저장\n",
    "  - 상품 가격 변화나 실시간 키워드 추적  \n",
    "- 자동화 시에는 로그 저장, 날짜 기록, 중복 제거 등도 함께 고려해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30927e16-88c6-48b7-bcff-96f4362599d5",
   "metadata": {},
   "source": [
    "## 핵심 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4a899-55aa-4ca1-9b32-9bd229ab4cfc",
   "metadata": {},
   "source": [
    "- **정적 크롤링**: `rvest`, `html_table`, `dplyr`  \n",
    "- **API 기반**: `httr::GET()`, `jsonlite::fromJSON()`  \n",
    "- **동적 페이지 처리**: `RSelenium`으로 JavaScript 인터랙션  \n",
    "- **로그인/세션 처리**: `httr::POST()`  \n",
    "- **자동화 실행**: `cronR`(Linux), `taskscheduleR`(Windows)  \n",
    "- 위 기술들을 종합하면 실질적인 데이터 수집 프로젝트를 구현할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156a3ab-f9f8-424f-95ba-7842bb0d1f0d",
   "metadata": {},
   "source": [
    "# Python을 활용한 웹 크롤링 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0709c-20c1-41ad-9476-c9e60009912c",
   "metadata": {},
   "source": [
    "## 정적 웹 크롤링 (`requests`, `BeautifulSoup`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa35590-d988-4ad4-816c-9449dfc1fe3d",
   "metadata": {},
   "source": [
    "- `requests` 패키지로 HTML 페이지를 요청하고, `BeautifulSoup`으로 원하는 데이터를 추출  \n",
    "- HTML 태그 및 클래스 구조를 기반으로 텍스트, 링크, 테이블 등 수집 가능  \n",
    "- `<table>` 형식의 데이터는 `pandas.read_html()` 또는 `BeautifulSoup` + 수작업 파싱으로 처리 가능  \n",
    "- `pandas`를 활용하여 데이터 정제 후 `to_csv()`로 저장 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582449d0-6c41-4101-9285-b92bb424b1f9",
   "metadata": {},
   "source": [
    "## API 기반 데이터 수집 (`requests`, `json` or `pandas`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb23b6e-6b80-4446-bbfa-7d1dd34d8f44",
   "metadata": {},
   "source": [
    "- REST API를 활용하면 JSON 또는 XML로 구조화된 데이터를 직접 가져올 수 있음  \n",
    "- `requests.get()`으로 API 호출, `response.json()` 또는 `json.loads()`로 파싱  \n",
    "- 데이터는 `pandas.DataFrame`으로 변환 후 분석 및 저장 가능  \n",
    "- HTML 크롤링보다 빠르고 안정적이며, 유지보수가 쉬움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8794f-6b3d-4d48-9f7a-8ff3a833af76",
   "metadata": {},
   "source": [
    "## JavaScript 기반 동적 크롤링 (`Selenium`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ab868-ba1e-4f3c-a661-50b0c46f5ca2",
   "metadata": {},
   "source": [
    "- JavaScript가 실행되어야 나타나는 콘텐츠는 `requests`나 `BeautifulSoup`으로는 수집 불가  \n",
    "- `Selenium`을 사용하면 실제 브라우저를 조작하듯 클릭, 스크롤 등 사용자 동작을 시뮬레이션 가능  \n",
    "- 동적 콘텐츠가 로드된 이후의 HTML을 `driver.page_source`로 가져와 파싱  \n",
    "- 무한스크롤, 버튼 클릭, 드롭다운 처리 등에 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e5b14-2990-4453-9ed5-b7c6217592d2",
   "metadata": {},
   "source": [
    "## 로그인 및 세션 유지 (`requests.Session()` 또는 `Selenium`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5d3ac-0ec6-422b-a05f-f4b1fc44c685",
   "metadata": {},
   "source": [
    "- 로그인 후 접근 가능한 페이지의 경우, 세션 쿠키를 유지하며 요청 필요  \n",
    "- `requests.Session()`을 사용해 로그인 요청을 보내고 이후 페이지 접근  \n",
    "- 또는 `Selenium`으로 로그인 과정을 자동화하고 이후 페이지 조작  \n",
    "- 단, CAPTCHA, OTP 등 인증 절차가 있으면 자동화가 제한될 수 있음 → 가능하면 API 사용 권장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d890f73-28e7-4628-ab8f-778b83d787b3",
   "metadata": {},
   "source": [
    "## 자동화 및 실전 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e57ef4-7bfe-4291-922e-996399d979d6",
   "metadata": {},
   "source": [
    "- 크롤링 작업을 자동화하려면 스케줄러 또는 배치 시스템이 필요  \n",
    "- Python에서는 `schedule`, `APScheduler`, `cron`(Linux), 작업 스케줄러(Windows) 등을 활용  \n",
    "- 예시 프로젝트:\n",
    "  - 매일 뉴스 기사 수집 및 CSV 저장\n",
    "  - 실시간 인기 키워드 또는 상품 가격 추적  \n",
    "- 자동화 시 로그 저장, 중복 제거, 날짜 기록 등의 예외 처리가 중요함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e05dfd-70f8-4605-ab6a-9e3ba505e845",
   "metadata": {},
   "source": [
    "## 핵심 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2095f8-a101-4ff5-b13b-8a22ada64056",
   "metadata": {},
   "source": [
    "- **정적 크롤링**: `requests`, `BeautifulSoup`, `pandas`  \n",
    "- **API 활용**: `requests`, `json`, `pandas`  \n",
    "- **동적 크롤링**: `Selenium`을 통한 JavaScript 제어  \n",
    "- **로그인/세션 처리**: `requests.Session()` 또는 `Selenium` 로그인 시퀀스  \n",
    "- **자동화**: `schedule`, `APScheduler`, `cron`, `task scheduler`  \n",
    "- 이들을 조합하면 실전 웹 크롤링 및 데이터 수집 시스템 구축 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec3e85-f620-4976-bce1-8f3c0afc8bea",
   "metadata": {},
   "source": [
    "좋아, 순서대로 진행할게.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 1. Python 버전 **영문 요약본** (마크다운 형식)\n",
    "\n",
    "```markdown\n",
    "# Web Crawling with Python – Summary\n",
    "\n",
    "## 1. Introduction to Web Crawling\n",
    "\n",
    "- **Web crawling**: Navigating across websites to collect data  \n",
    "- **Web scraping**: Extracting specific information from individual web pages  \n",
    "- Categorized into **static (HTML-based)** and **dynamic (JavaScript-based)** crawling\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Static Web Crawling (`requests`, `BeautifulSoup`)\n",
    "\n",
    "- Use `requests` to fetch HTML pages and `BeautifulSoup` to parse them  \n",
    "- Extract text, links, or tables based on HTML tags and class structures  \n",
    "- Use `pandas.read_html()` for tables, or manually parse with `BeautifulSoup`  \n",
    "- Clean data using `pandas` and save with `.to_csv()`\n",
    "\n",
    "---\n",
    "\n",
    "## 3. API-Based Data Collection (`requests`, `json`, `pandas`)\n",
    "\n",
    "- REST APIs provide structured data (JSON/XML) directly  \n",
    "- Use `requests.get()` and `response.json()` or `json.loads()` to parse responses  \n",
    "- Convert JSON to `pandas.DataFrame` for analysis and export  \n",
    "- More stable and faster than HTML scraping\n",
    "\n",
    "---\n",
    "\n",
    "## 4. JavaScript-Based Dynamic Crawling (`Selenium`)\n",
    "\n",
    "- JavaScript-rendered pages can’t be scraped with `requests` or `BeautifulSoup`  \n",
    "- Use `Selenium` to simulate browser actions (clicks, scrolling, etc.)  \n",
    "- Extract final content using `driver.page_source` after the page loads  \n",
    "- Ideal for infinite scrolls, pop-ups, and dynamic interactions\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Login and Session Management (`requests.Session()` or `Selenium`)\n",
    "\n",
    "- Use `requests.Session()` to maintain cookies and login credentials  \n",
    "- For complex interactions, automate login via `Selenium`  \n",
    "- Beware of CAPTCHA or multi-factor authentication – prefer APIs if available\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Automation and Real-World Projects\n",
    "\n",
    "- Use schedulers or job runners to automate crawling  \n",
    "- Tools: `schedule`, `APScheduler`, `cron` (Linux), Task Scheduler (Windows)  \n",
    "- Example use cases:\n",
    "  - Daily news scraping into CSV  \n",
    "  - Monitoring product price trends or real-time keywords  \n",
    "- Include logging, duplicate checks, and date-stamping for reliability\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Key Takeaways\n",
    "\n",
    "- **Static scraping**: `requests`, `BeautifulSoup`, `pandas`  \n",
    "- **API collection**: `requests`, `json`, `pandas`  \n",
    "- **Dynamic scraping**: `Selenium` for full browser interaction  \n",
    "- **Login/session**: `requests.Session()` or automated login via `Selenium`  \n",
    "- **Automation**: `schedule`, `APScheduler`, `cron`, Task Scheduler  \n",
    "- Together, these tools can power scalable, real-world web crawling workflows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 2. **R vs Python 웹 크롤링 비교표** (한국어)\n",
    "\n",
    "| 항목 | R | Python |\n",
    "|------|---|--------|\n",
    "| **정적 크롤링** | `rvest`, `html_table`, `dplyr` | `requests`, `BeautifulSoup`, `pandas` |\n",
    "| **표 데이터 처리** | `html_table()` + `dplyr` | `pandas.read_html()` 또는 직접 파싱 |\n",
    "| **API 요청** | `httr::GET()` + `jsonlite::fromJSON()` | `requests.get()` + `response.json()` 또는 `json.loads()` |\n",
    "| **동적 크롤링** | `RSelenium` | `Selenium` |\n",
    "| **로그인 처리** | `httr::POST()` + 세션 유지 | `requests.Session()` 또는 `Selenium` |\n",
    "| **자동화 도구** | `cronR`, `taskscheduleR` | `cron`, `APScheduler`, `schedule`, Windows Task Scheduler |\n",
    "| **장점** | 통계 및 데이터 처리에 강함 | 생태계 다양, 커뮤니티 방대, 크롤링 라이브러리 풍부 |\n",
    "| **단점** | 환경 설정이 까다로운 편, 실시간 처리 부족 | 웹 개발 요소와 통합 시 학습 곡선 있음 |\n",
    "\n",
    "> ✅ 요약:  \n",
    "> - **R은 분석 친화적인 환경에서 간단한 크롤링 & 정제 작업에 강하고**,  \n",
    "> - **Python은 대규모 자동화, 복잡한 웹 구조 대응, API 중심 작업에 더 적합**.\n",
    "\n",
    "---\n",
    "\n",
    "필요하면 이 비교표를 이미지, PDF, 발표 슬라이드용 구조로도 만들어줄 수 있어.  \n",
    "다음으로 어떤 포맷이 필요해? 또는 이 내용을 어디에 활용할 예정이야?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
